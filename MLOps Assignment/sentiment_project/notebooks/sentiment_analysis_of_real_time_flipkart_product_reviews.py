# -*- coding: utf-8 -*-
"""Sentiment Analysis of Real-time Flipkart Product Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u33v2CtWTrRqp3elYk3MAsYjyHkC2zm5

# Phase 1
"""

import os
import pandas as pd
import re
import streamlit as st
import nltk
import joblib

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
joblib.dump(lr, "sentiment_model.pkl")
joblib.dump(tfidf, "tfidf_vectorizer.pkl")

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.util import ngrams
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, classification_report
from sklearn.naive_bayes import MultinomialNB
from collections import Counter

from google.colab import files
files.upload()

os.listdir("/content")

df = pd.read_csv("data.csv")
df.head()

df.columns

df.rename(columns={
    'Review text': 'review_text',
    'Ratings': 'rating'
}, inplace=True)

df.head()

df = df[['review_text', 'rating']]
df.head()

df.isnull().sum()
df.dropna(inplace=True)

df = df[df['rating'] != 3]

df['sentiment'] = df['rating'].apply(
    lambda x: 'positive' if x >= 4 else 'negative'
)

df.head()

"""# Phase 2"""

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # convert to lowercase
    text = text.lower()

    # remove special characters & numbers
    text = re.sub(r'[^a-z\s]', '', text)

    # tokenize and remove stopwords + lemmatize
    words = text.split()
    words = [lemmatizer.lemmatize(word)
             for word in words
             if word not in stop_words]

    return " ".join(words)

df['clean_review'] = df['review_text'].apply(clean_text)
df.head()

df[['review_text', 'clean_review']].head(3)

"""# Phase 3"""

X = df['clean_review']
y = df['sentiment']

X_train, X_test, y_train, y_test = train_test_split(
    X,
    y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

tfidf = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1, 2)
)

X_train_tfidf = tfidf.fit_transform(X_train)
X_test_tfidf = tfidf.transform(X_test)

X_train_tfidf.shape, X_test_tfidf.shape

"""# Phase 4"""

lr = LogisticRegression(max_iter=1000)

lr.fit(X_train_tfidf, y_train)

y_pred_lr = lr.predict(X_test_tfidf)

f1_lr = f1_score(y_test, y_pred_lr, pos_label='positive')
print("Logistic Regression F1-score:", f1_lr)

nb = MultinomialNB()

nb.fit(X_train_tfidf, y_train)

y_pred_nb = nb.predict(X_test_tfidf)

f1_nb = f1_score(y_test, y_pred_nb, pos_label='positive')
print("Naive Bayes F1-score:", f1_nb)

print("Logistic Regression Report:\n")
print(classification_report(y_test, y_pred_lr))

print("Naive Bayes Report:\n")
print(classification_report(y_test, y_pred_nb))

"""# Phase 5 : Already done in header (Saved Best Module)

# Phase 6
"""

negative_reviews = df[df['sentiment'] == 'negative']
negative_reviews.head()

all_words = " ".join(negative_reviews['clean_review']).split()
word_freq = Counter(all_words)

word_freq.most_common(20)

bigrams = list(ngrams(all_words, 2))
Counter(bigrams).most_common(15)

!streamlit run app.py